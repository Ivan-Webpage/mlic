# 「蝦皮爬蟲」最詳細手把手教學｜商品資料＋留言評論一次滿足【2023最新版】
## 課程介紹
> [建議您可以先看「Json爬蟲教學－Google趨勢搜尋」課程，會更了解蝦皮如何爬喔！](/classification/crawler_king/85)

蝦皮橫掃台灣的店商產業，搶佔市佔率的結果，當年的Yahoo拍賣、露天拍賣都不是對手。或許您或您的企業也有經營電商生意，對於蝦皮的進軍肯定是膽戰心驚。若換個角度來想，蝦皮也是統整了許多「競爭對手」的平台，若我們想模仿產業內的行銷方式，或者想窺探競爭對手的動態；又或者是，你的商品單價過高，不適合在蝦皮上架，但蝦皮內的某些消費者需求，或許能應用到自己的品牌，因此爬下蝦皮對於產業的窺探有不可忽視的效果。

## 可交付成果
許多網路的文章都有蝦皮爬蟲的教學，但實際執行時就會發現爬蟲被官方擋住了。本文章經實驗最高可得到7萬比的商品資料，以及這些商品所有的留言更是破好幾百萬，這個結果是因為作者主動停下爬蟲（因為檔案過大），因此利用本文章的工具取得蝦皮中單一品項的所以資料是有可能的。

可交付成果有兩個主要檔案，命名方式分別為「<strong>XXX_商品資料.csv</strong>」與「<strong>XXX_留言資料.csv</strong>」，其內容為每個商品的所有資訊與留言資料，爬蟲的過程依照需求的不同，可能會爬到好幾夜之久，為了怕途中任何意外導致功歸一匱，因此每爬完一頁（50個商品）就會存檔記錄一次。
![成果：留言資料](https://cdn-images-1.medium.com/max/1200/1*M1OiQQ0O-599iC9Pem-aug.png)
![成果：商品資料](https://cdn-images-1.medium.com/max/1200/1*rSAciO0mzW_fXvIrcp_9Hg.png)

## 開始爬蟲
### 基本參數設定
本次的課程以服裝產業的「花襯衫」這個品項為例子，由於產品過多，因此先設定爬下50頁商品資料作為範例。至於變數my_headers內的資料，為封包需要用到的Header資料為何會那麼長，可以參考「[Html爬蟲Post實戰－UberEats](/classification/crawler_king/88)」課程中有詳細的解說。

```python
keyword = '花襯衫'
page = 5
ecode = 'utf-8-sig'
my_headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36',
    'if-none-match-': '55b03-6d83b58414a54cb5ffbe81099940f836'
    }  
```
### 設定商品的「留言資料」方法
在課程「[Json爬蟲教學－Google趨勢搜尋](/classification/crawler_king/85)」中已經教您如何取得API資料，再利用requests.get()方法取得文字內容後，需要將一些特殊的字元進行取代後，才能利用 json.loads 轉成Json資料格式。以下方法給予「商品ID」與「店家ID」，即可取得該商品的買家留言。

```python
# 進入每個商品，抓取買家留言
def goods_comments(item_id, shop_id):
    url = 'https://shopee.tw/api/v4/item/get_ratings?filter=0&flag=1&itemid='+ str(item_id) + '&limit=50&offset=0&shopid=' + str(shop_id) + '&type=0'
    r = requests.get(url,headers = my_headers)
    st= r.text.replace("\\n","^n")
    st=st.replace("\\t","^t")
    st=st.replace("\\r","^r")
    
    gj=json.loads(st)
    return gj['data']['ratings']
```
### 設定商品的「商品資料」方法
此部分在2022年的第四季左右，發現此API無法再拉到商品的詳細資料，經過檢查後發現蝦皮新增了防爬蟲機制，因此作者改用[seleniumwire](https://pypi.org/project/selenium-wire/)直接撈取封包，更詳細的Debug心路歷程，請見文章最後的「Debug日誌」。
```python
# 進入每個商品，抓取賣家更細節的資料（商品文案、SKU）
def goods_detail(url, item_id, shop_id):
    # 2022/12/29 ivan，因shopee API新增了防爬蟲機制，header中多了「af-ac-enc-dat」參數，因解析不出此參數如何製成，只能使用土法煉鋼，一頁一頁進去攔封包
    driver.get(url) # 需要到那個頁面，才能度過防爬蟲機制
    time.sleep(random.randint(10,20))
    getPacket = ''
    for request in driver.requests:
        if request.response:
            # 挑出商品詳細資料的json封包
            if 'https://shopee.tw/api/v4/item/get?itemid=' + str(item_id) + '&shopid=' + str(shop_id) in request.url:
                # 此封包是有壓縮的，因此需要解壓縮
                getPacket = zlib.decompress(
                    request.response.body,
                    16+zlib.MAX_WBITS
                    )
                break
    if getPacket != '':
        gj=json.loads(getPacket)
        return gj['data']
    else:
        return getPacket

```
### 為何使用Selenium與設定
接下來使用Selenium來進入蝦皮網站，若還沒了解 Selenium 的您，可以參考「[Selenium介紹](/classification/crawler_king/92)」課程。或許您在搜尋許多蝦皮爬蟲時，在許多網路文章都會使用前面段落所使用的API來爬蟲，一樣可以取得資料，您可能會問那還何必使用Selenium來爬蟲呢？這不是殺雞焉用牛刀嗎？其實不然！

在作者自己的實驗中，若爬到約50頁左右的蝦皮頁面開始，蝦皮API封包內的json 資料就會開始有缺漏，明顯是蝦皮官方為了防止機器人行為而進行的防護行為；反之若使用Selenium 來進行滾動頁面，雖然速度會慢上許多，但經過實驗證得，這樣並不會被官方認定為機器人，因此蝦皮API封包內的json 資料還是會一樣完整。

由於在2022年底蝦皮的API更新，因此本爬蟲改使用[seleniumwire](https://pypi.org/project/selenium-wire/)來進行爬取，因此需要先安裝[seleniumwire](https://pypi.org/project/selenium-wire/)，安裝指令如下：
```
pip install selenium-wire
```

### 資料容器準備
整個爬蟲的過程會利用time.time() 的方法來計時，方便之後調整爬蟲的效率。另外準備了container_product 與container_comment 兩個變數。分別用來存放商品的詳細資料與每個商品對應的留言。
在2022/12的程式碼修整中，也將`options.add_argument('blink-settings=imagesEnabled=false') `加入程式碼，此舉能讓
```python
# 自動下載ChromeDriver
service = ChromeService(executable_path=ChromeDriverManager().install())

# 關閉通知提醒
options = webdriver.ChromeOptions()
prefs = {"profile.default_content_setting_values.notifications" : 2}
options.add_experimental_option("prefs",prefs)
# 不載入圖片，提升爬蟲速度
options.add_argument('blink-settings=imagesEnabled=false') 

# 開啟瀏覽器
driver = webdriver.Chrome(service=service, chrome_options=options)
time.sleep(random.randint(5,10))

# 開啟網頁
driver.get('https://shopee.tw/search?keyword=' + keyword )
time.sleep(random.randint(10,20))
```

### 請先手動登入
> 2023/04/20新增此項步驟

由於蝦皮不允許匿名使用者登入，因此在往下執行之前，請先在瀏覽器中手動登入自己的蝦皮帳號密碼。蝦皮此舉筆者猜測，若蝦皮偵測到該帳號的行為異常，就會註記，從此你的帳號有事沒事就會被要求機器人驗證，這是蝦皮防範機器人的一大手段。

由此可知，我們的帳號是有限的資源，沒事不要去給蝦皮貼上標籤，因此程式碼中有許多睡覺（time.sleep()）的指令，就是為了要降低爬蟲頻率，以免被註記。您可以將睡覺時間延長保護自己帳號安全，但不建議縮短。

### 開始每頁商品的爬蟲

![取得商品的示意圖](https://cdn-images-1.medium.com/max/1200/1*ChrqpHRuFbjWuPUHcos6xA.png)
我們在「基本參數設定」的步驟時已經設定了page 這個變數，這個變數決定我們要爬幾頁資料（一頁50個商品），因此這層的迴圈就是一頁一頁進去取得商品資料。
若您是![行銷搬進大程式](https://marketingliveincode.com/home)的忠實粉絲，您會發現現在的蝦皮爬蟲程式碼與過往有些不同。以前的爬蟲方式，是搜尋商品後，每頁50個商品一個一個進去抓所有資料，每抓完一頁儲存一次資料。這個方式有一個很大的麻煩，是在於每一頁的爬蟲時間相當的久，在這個過程中有任何不可抗拒因素造成程式終止，那之前付出的時間都浪費了。
再加上2023年後蝦皮的防爬蟲機制又一次顯著的升級，對於可疑的帳戶會不定時的要求機器人驗證，此舉無疑對爬蟲是一個致命的打擊，也會時常造成爬蟲中斷。因此現在的爬蟲方式變成是，先記錄下搜尋到的每個商品連結，之後再回來慢慢一個個爬，這樣的差別在哪裏呢？那就接著看下去吧！
```python
for i in range(int(page)):

    # 準備用來存放資料的陣列
    itemid = []
    shopid =[]
    name = []
    brand = [] ＃以下變數太多因此省略，完整程式碼請見文章最下方...
    
    driver.get('https://shopee.tw/search?keyword=' + keyword + '&amp;page=' + str(i))
```

#### 滾動頁面
![滾動時可以發現，商品資料會一一載入](https://cdn-images-1.medium.com/max/1200/1*3-2_YdQC_e18Gbgs_59q6g.png)
蝦皮也是屬於動態式的網站，因此畫面必須要向下滾動，才會觸發而取得所有50個商品資料。
```python
# 滾動頁面
    for scroll in range(6):
        driver.execute_script('window.scrollBy(0,1000)')
        time.sleep(2)
        
```
#### 取得商品的「商品ID」、「商家ID」、「商品名稱」
![取得連結即可獲得商品的「商品ID」、「商家ID」、「商品名稱」](https://cdn-images-1.medium.com/max/1200/1*BNoPlFzIBjQSmqHr6x8JVQ.png")
由上圖可以發現，商品的「商品ID」、「商家ID」、「商品名稱」都在連結裏頭，因此會特別將標籤內的href 標籤爬下來。
```python
#取得商品內容
for block in driver.find_elements(by=By.XPATH, value='//*[@data-sqe="item"]'):
    # 將整個網站的Html進行解析
    soup = BeautifulSoup(block.get_attribute('innerHTML'), "html.parser").find('a')
    # 商品ID、商家ID、商品連結
    getID = soup.get('href')
    theitemid = int((getID[getID.rfind('.')+1:getID.rfind('?')]))
    theshopid = int(getID[ getID[:getID.rfind('.')].rfind('.')+1 :getID.rfind('.')]) 
    
    # 先整理標籤
    get_parent = soup.find('div',{"data-sqe":"name"}).parent.find_all("div", recursive=False)
    
    # 商品名稱
    if len(get_parent) >0: # 確認有資料再進行
        getname = get_parent[0].find('div').text
    else:
        print('抓不到資料，直接是空的')
        continue # 沒抓到這個商品就別爬了
```
蝦皮商品的連結格式如下圖：
```https://shopee.tw/【商品的名稱】-i.【商品ID】.【店家ID】```

#### 取得商品「價格」
![資料有時會不完整](https://cdn-images-1.medium.com/max/1200/1*Y1VRc5ylic73qMGV1J4C4w.png)
商品的「價格」資料因為有時會不完整，因此需要使用IF判斷式來防止程式出錯。取得的價格必須要以數字的型態儲存，方便之後資料計算方便，因此裡面所有的「非數值型資料」都需要取代掉。
```python
#價格
#價格
if len(get_parent) >1: # 確認有資料再進行
    getSpan = get_parent[1].find_all('span')
    counter = []
    for j in getSpan:
        theprice = j.text
        theprice = theprice.replace('萬','')
        theprice = theprice.replace('$','')
        theprice = theprice.replace(',','')
        theprice = theprice.replace(' ','')

        if theprice != '':
            counter.append(int(theprice))

    # 到這邊確認都有抓到資料，才將它塞入陣列，否則可能會有缺漏
    link.append("https://shopee.tw"+getID)
    itemid.append(theitemid)
    shopid.append(theshopid)
    name.append(getname)
    price.append(sum(counter)/len(counter))
else:
    print('抓不到價格資料')
    continue # 沒抓到這個商品就別爬了
```

#### 先儲存商品資料的初版

抓取完指定的頁數後，便可以開始整理抓到的所有資料。因為我們現在的模式變成是先抓到商品連結，之後再來補資料，因此其他欄位的部分就先補空值`N`，下一個階段再將內容補齊。
```python
dic = {
    '商品ID':itemid,
    '賣家ID':shopid,
    '商品名稱':name,
    '商品連結':link,
    '價格':price,
    '品牌': [ None for x in range(len(itemid)) ] ,
    '存貨數量':[ None for x in range(len(itemid)) ] ,
    '商品文案':[ None for x in range(len(itemid)) ] ,
    '上架時間':[ None for x in range(len(itemid)) ] ,
    '折數':[ None for x in range(len(itemid)) ] ,
    '可否搭配購買':[ None for x in range(len(itemid)) ] ,
    '可否大量批貨購買':[ None for x in range(len(itemid)) ] ,
    '選項':[ None for x in range(len(itemid)) ] ,
    '歷史銷售量':[ None for x in range(len(itemid)) ] ,
    '可否分期付款':[ None for x in range(len(itemid)) ] ,
    '是否官方賣家帳號':[ None for x in range(len(itemid)) ] ,
    '是否可預購':[ None for x in range(len(itemid)) ] ,
    '喜愛數量':[ None for x in range(len(itemid)) ] ,
    '商家地點':[ None for x in range(len(itemid)) ] ,
    'SKU':[ None for x in range(len(itemid)) ] ,
    '評價數量':[ None for x in range(len(itemid)) ] ,
    '五星':[ None for x in range(len(itemid)) ] ,
    '四星':[ None for x in range(len(itemid)) ] ,
    '三星':[ None for x in range(len(itemid)) ] ,
    '二星':[ None for x in range(len(itemid)) ] ,
    '一星':[ None for x in range(len(itemid)) ] ,
    '評分':[ None for x in range(len(itemid)) ] ,
    '資料已完整爬取':[ False for x in range(len(itemid)) ] ,
}
pd.DataFrame(dic).to_csv(keyword +'_商品資料.csv', encoding = ecode, index=False)

tEnd = time.time()#計時結束
totalTime = int(tEnd - tStart)
minute = totalTime // 60
second = totalTime % 60
print('資料儲存完成，花費時間（約）： ' + str(minute) + ' 分 ' + str(second) + '秒')
```

### 請求商品詳細資料
請求「設定商品的「商品資料」方法」步驟的方法，取得所有的Json資料，並整理到所有的變數中，方便等等轉乘DataFrame 資料型態。您可以發現，在程式的一開始先將剛剛整理完的商品資料檔案引入，並且檢查後方欄位「資料已完整爬取」。
這項設計的目的就是，若在爬取商品資料的過程中出現不可抗拒因素，導致爬蟲停下來，在重新執行這段時，會檢查欄位「資料已完整爬取」為`False`的欄位，等於說爬過的商品就不會在爬了，這樣能使程式延續執行。
```python
print('---------- 開始進行爬蟲 ----------')
tStart = time.time()#計時開始

# 2023/04/20 先取得之前爬下來的紀錄
getData = pd.read_csv(keyword +'_商品資料.csv')
for i in tqdm(range(len(getData))):
    data = []
    # 2023/04/20 備標註已經抓過的，就不用再抓了，這樣就算之前爬到一半被中斷，也不會努力付諸東流
    if getData.iloc[i]['資料已完整爬取']==True:
        continue
```

#### 請求蝦皮API

在檢查確認該商品還沒被爬過後，就可以藉由先前爬下的商品連結、ID資料，來對蝦皮的API進行請求了。由於可能會有商品下架、API請求失敗等問題，因此加入if判斷式，來檢查是否有抓取成功。
```python
data.append(getData.iloc[i]['商品ID'])
data.append(getData.iloc[i]['賣家ID'])
data.append(getData.iloc[i]['商品名稱'])
data.append(getData.iloc[i]['商品連結'])
data.append(getData.iloc[i]['價格'])
#請求商品詳細資料
itemDetail = goods_detail(url = data[3], item_id = data[0], shop_id = data[1])

# 抓不到資料就先跳過
if itemDetail == '':
    print('抓不到商品詳細資料...\n')
    continue
```

#### 寫入完整商品資料
將API拿到的結果記錄下來。這裡必須要注意，由於不一定每個商品都有「SKU」，因此在整理之前必須要先檢查是否有資料。
```python
data.append(itemDetail['brand'])# 品牌
data.append(itemDetail['stock'])# 存貨數量
data.append(itemDetail['description'])# 商品文案
data.append(itemDetail['ctime'])# 上架時間
data.append(itemDetail['discount'])# 折數
data.append(itemDetail['can_use_bundle_deal'])# 可否搭配購買
data.append(itemDetail['can_use_wholesale'])# 可否大量批貨購買
data.append(itemDetail['tier_variations'])# 可否分期付款
data.append(itemDetail['historical_sold'])# 歷史銷售量
data.append(itemDetail['is_cc_installment_payment_eligible'])# 可否分期付款
data.append(itemDetail['is_official_shop'])# 是否官方賣家帳號
data.append(itemDetail['is_pre_order'])# 是否可預購
data.append(itemDetail['liked_count'])# 喜愛數量
data.append(itemDetail['shop_location'])# 商家地點
#SKU
all_sku=[]
for sk in itemDetail['models']:
    all_sku.append(sk['name'])
data.append(all_sku)# SKU
data.append(itemDetail['cmt_count'])# 評價數量
data.append(itemDetail['item_rating']['rating_count'][5])# 五星
data.append(itemDetail['item_rating']['rating_count'][4])# 四星
data.append(itemDetail['item_rating']['rating_count'][3])# 三星
data.append(itemDetail['item_rating']['rating_count'][2])# 二星
data.append(itemDetail['item_rating']['rating_count'][1])# 一星
data.append(itemDetail['item_rating']['rating_star'])# 評分
data.append(True)# 資料已完整爬取
```

#### 每取得一個商品就存檔

此舉也是為了防止爬到一半功歸一匱。值得注意的是，為了讓爬蟲的行為不要那麼頻繁，且模仿真人，也設計每爬五個商品，會多額外休息一段時間。
```python
getData.iloc[i] = data #塞入所有資料
getData.to_csv(keyword +'_商品資料.csv', encoding = ecode, index=False)

time.sleep(random.randint(45,70)) # 休息久一點

# 每爬5個商品，會再有一次更長的休息
if i%5 == 0 :
    time.sleep(random.randint(30,150)) 

```
### 取得商品的「留言資料」
請求「設定商品的「留言資料」方法」步驟的方法，取得所有的Json資料。由於Json檔案中的['comment']、['model_name']，不一定每個留言都有，因此使用try error的方式來寫。
```python
print('---------- 開始進行爬蟲 ----------')
tStart = time.time()#計時開始
container_comment = pd.DataFrame()
getData = pd.read_csv(keyword +'_留言資料.csv')
for i in range(len(getData)):
    
    # 消費者評論詳細資料
    iteComment = goods_comments(item_id = theitemid, shop_id = theshopid)
    userid = [] #使用者ID
    anonymous = [] #是否匿名
    commentTime = [] #留言時間
    is_hidden = [] #是否隱藏
    orderid = [] #訂單編號
    comment_rating_star = [] #給星
    comment = [] #留言內容
    product_SKU = [] #商品規格
    
    for comm in iteComment:
        userid.append(comm['userid'])
        anonymous.append(comm['anonymous'])
        commentTime.append(comm['ctime'])
        is_hidden.append(comm['is_hidden'])
        orderid.append(comm['orderid'])
        comment_rating_star.append(comm['rating_star'])
        try:
            comment.append(comm['comment'])
        except:
            comment.append(None)
        
        p=[]
        for pro in comm['product_items']:
            try:
                p.append(pro['model_name'])
            except:
                p.append(None)
                
        product_SKU.append(p)

```
#### 整理所有「商品」與「留言」資料
整理上述的所有資料，並打包成DataFrame 資料格式。在整個For 迴圈的執行途中，為了防止任何突發意外，導致迴圈停止資料消失，因此在每一頁爬蟲結束時會儲存檔案一次，並整理成CSV檔案。
```python
commDic = {
    '商品ID':[ theitemid for x in range(len(iteComment)) ],
    '賣家ID':[ theshopid for x in range(len(iteComment)) ],
    '商品名稱':[ getname for x in range(len(iteComment)) ],
    '價格':[ int(theprice) for x in range(len(iteComment)) ],
    '使用者ID':userid,
    '是否匿名':anonymous,
    '留言時間':commentTime,
    '是否隱藏':is_hidden,
    '訂單編號':orderid,
    '給星':comment_rating_star,
    '留言內容':comment,
    '商品規格':product_SKU
    }

container_comment = pd.concat([container_comment,pd.DataFrame(commDic)], axis=0)
container_comment.to_csv(keyword +'_留言資料.csv', encoding = ecode, index=False)

```
#### 存檔
經過了漫長的執行，最後儲存檔案，並結算這次執行的總時間。
```python
tEnd = time.time()#計時結束
totalTime = int(tEnd - tStart)
minute = totalTime // 60
second = totalTime % 60
print('資料儲存完成，花費時間（約）： ' + str(minute) + ' 分 ' + str(second) + '秒')

driver.close()
```

## Debug日誌
### 蝦皮V4 API阻擋爬蟲
> 再次感謝[許譽騰](https://www.facebook.com/profile.php?id=100002303638013)同學協助排除此bug！
在2022第四季，陸續有學員發現蝦皮的API請求商品資料時，會回傳以下json結果：
```json
{
    "challenge_type": 1,
    "captcha": {
        "scene": "crawler_item"
    },
    "captcha_extra_info": {
        "tracking_id": "95961fb7-6ad3-494e-80fb-13646c9dd347"
    },
    "action_type": 3,
    "error": 90309999,
    "is_customized": false,
    "is_login": false,
    "platform": 0,
    "report_extra_info": ""
}
```

看到「crawler_item」心頭一陣涼風，蝦皮是怎麼知道我是爬蟲的？而後開始排查header的參數，雖然新增了不少餐數與cookies，但最關鍵的參數莫過於以下：
```json
{
    "af-ac-enc-dat": "AAcyLjQuMS0yAAABhELlOE8AAAtvAkAAAAAAAAAAAOYyhFAbVQMMpIKa2+...太長了後面省略"
}
```
當我將這個參數改為null後重新請求，網頁就能夠顯示了，但仍然不會給我商品的json資料，看來在這其中，除了跟cookie有交互檢查外，也有包含商品的參數在裡面，因此這個參數仍然不可或缺。
```json
{
    "af-ac-enc-dat": "null"
}
```
最好的解決方法，是破解「af-ac-enc-dat」這個參數是如何生成的，但這條逆向工程的路遙不可及（至少對我是），正在手足無措時，收到[行銷搬進大程式](https://marketingliveincode.com/)的同學[許譽騰](https://www.facebook.com/profile.php?id=100002303638013)來信，建議我可以使用[seleniumwire](https://pypi.org/project/selenium-wire/)的方式抓取生成出來的「af-ac-enc-dat」，好方法！立馬手刀實驗！
經過實驗，發現確實能撈到生成出來的「af-ac-enc-dat」，但既然能生成出來的「af-ac-enc-dat」，代表整個封包也一定被撈到了？（有玩過[wireshark](https://www.wireshark.org/download.html)的讀者一定聽得懂）。經過幾個小時的摸索，發現[seleniumwire](https://pypi.org/project/selenium-wire/)果真有收錄封包的內容在「request.response.body」裡面，才會以以下的程式碼，來取得封包內容：
```python
getPacket = ''
for request in driver.requests:
    if request.response:
        # 挑出商品詳細資料的json封包
        if 'https://shopee.tw/api/v4/item/get?itemid=' + str(item_id) + '&shopid=' + str(shop_id) in request.url:
            # 此封包是有壓縮的，因此需要解壓縮
            getPacket = zlib.decompress(
                request.response.body,
                16+zlib.MAX_WBITS
                )
            break
```


### 蝦皮防爬蟲機制升級，加入機器人驗證
> 2023年起，在蝦皮搜尋商品前，都會被要求一定要先登入蝦皮。並且開始有同學爬到一半，被要求機器人驗證

這個狀況是非常困擾的。首先是搜尋前輩要求一定要登入蝦皮。此舉很明顯是要「實名制」，當登入後若檢測到該帳號的行為不尋常，此帳號並不會直接封鎖，而是列為警示，從此您的帳號在瀏覽商品的途中，很常會被要求進行機器人驗證。

機器人驗證在現在AI的大環境下是有解方的，但不管事使用一些外部AI服務，或者是自己訓練一個AI來破解機器人驗證，都是成本非常高的一件事。因此我們也並沒有要正面迎戰機器人驗證的意思。

應對這次蝦皮的防禦升級，我將整個程式碼的邏輯大修一番，主要是加上了「資料已完整爬取」欄位。整個爬蟲的邏輯會變成：
1. 手動登入蝦皮。
2. 搜尋關鍵字相關商品，先記錄每一個商品的連結，並都標註「尚未爬取」。
3. 利用前一步完成的結果，美個商品的連結進去抓資料，抓成功會標記為「已經爬取」。此舉若爬到一半因故停下來，可以接著繼續爬。
4. 最後，使用每個商品的ID，去抓該商品的買賣留言。 
