# 「蝦皮爬蟲」最詳細手把手教學｜商品資料＋留言評論一次滿足【2023最新版】
## 課程介紹
> [建議您可以先看「Json爬蟲教學－Google趨勢搜尋」課程，會更了解蝦皮如何爬喔！](/classification/crawler_king/85)
蝦皮橫掃台灣的店商產業，搶佔市佔率的結果，當年的Yahoo拍賣、露天拍賣都不是對手。或許您或您的企業也有經營電商生意，對於蝦皮的進軍肯定是膽戰心驚。若換個角度來想，蝦皮也是統整了許多「競爭對手」的平台，若我們想模仿產業內的行銷方式，或者想窺探競爭對手的動態；又或者是，你的商品單價過高，不適合在蝦皮上架，但蝦皮內的某些消費者需求，或許能應用到自己的品牌，因此爬下蝦皮對於產業的窺探有不可忽視的效果。

## 可交付成果
許多網路的文章都有蝦皮爬蟲的教學，但實際執行時就會發現爬蟲被官方擋住了。本文章經實驗最高可得到7萬比的商品資料，以及這些商品所有的留言更是破好幾百萬，這個結果是因為作者主動停下爬蟲（因為檔案過大），因此利用本文章的工具取得蝦皮中單一品項的所以資料是有可能的。

可交付成果有兩個主要檔案，命名方式分別為「<strong>XXX_商品資料.csv</strong>」與「<strong>XXX_留言資料.csv</strong>」，其內容為每個商品的所有資訊與留言資料，爬蟲的過程依照需求的不同，可能會爬到好幾夜之久，為了怕途中任何意外導致功歸一匱，因此每爬完一頁（50個商品）就會存檔記錄一次。
![成果：留言資料](https://cdn-images-1.medium.com/max/1200/1*M1OiQQ0O-599iC9Pem-aug.png)
![成果：商品資料](https://cdn-images-1.medium.com/max/1200/1*rSAciO0mzW_fXvIrcp_9Hg.png)

## 開始爬蟲
### 基本參數設定
本次的課程以服裝產業的「花襯衫」這個品項為例子，由於產品過多，因此先設定爬下50頁商品資料作為範例。至於變數my_headers內的資料，為封包需要用到的Header資料為何會那麼長，可以參考「[Html爬蟲Post實戰－UberEats](/classification/crawler_king/88)」課程中有詳細的解說。

```python
keyword = '花襯衫'
page = 5
ecode = 'utf-8-sig'
my_headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36',
    'if-none-match-': '55b03-6d83b58414a54cb5ffbe81099940f836'
    }  
```
### 設定商品的「留言資料」方法
在課程「[Json爬蟲教學－Google趨勢搜尋](/classification/crawler_king/85)」中已經教您如何取得API資料，再利用requests.get()方法取得文字內容後，需要將一些特殊的字元進行取代後，才能利用 json.loads 轉成Json資料格式。以下方法給予「商品ID」與「店家ID」，即可取得該商品的買家留言。

```python
# 進入每個商品，抓取買家留言
def goods_comments(item_id, shop_id):
    url = 'https://shopee.tw/api/v4/item/get_ratings?filter=0&flag=1&itemid='+ str(item_id) + '&limit=50&offset=0&shopid=' + str(shop_id) + '&type=0'
    r = requests.get(url,headers = my_headers)
    st= r.text.replace("\\n","^n")
    st=st.replace("\\t","^t")
    st=st.replace("\\r","^r")
    
    gj=json.loads(st)
    return gj['data']['ratings']
```
### 設定商品的「商品資料」方法
此部分在2022年的第四季左右，發現此API無法再拉到商品的詳細資料，經過檢查後發現蝦皮新增了防爬蟲機制，因此作者改用[seleniumwire](https://pypi.org/project/selenium-wire/)直接撈取封包，更詳細的Debug心路歷程，請見文章最後的「Debug日誌」。
```python
# 進入每個商品，抓取賣家更細節的資料（商品文案、SKU）
def goods_detail(url, item_id, shop_id):
    # 2022/12/29 ivan，因shopee API新增了防爬蟲機制，header中多了「af-ac-enc-dat」參數，因解析不出此參數如何製成，只能使用土法煉鋼，一頁一頁進去攔封包
    driver.get(url) # 需要到那個頁面，才能度過防爬蟲機制
    time.sleep(random.randint(10,20))
    getPacket = ''
    for request in driver.requests:
        if request.response:
            # 挑出商品詳細資料的json封包
            if 'https://shopee.tw/api/v4/item/get?itemid=' + str(item_id) + '&shopid=' + str(shop_id) in request.url:
                # 此封包是有壓縮的，因此需要解壓縮
                getPacket = zlib.decompress(
                    request.response.body,
                    16+zlib.MAX_WBITS
                    )
                break
    if getPacket != '':
        gj=json.loads(getPacket)
        return gj['data']
    else:
        return getPacket

```
### 為何使用Selenium與設定
接下來使用Selenium來進入蝦皮網站，若還沒了解 Selenium 的您，可以參考「[Selenium介紹](/classification/crawler_king/92)」課程。或許您在搜尋許多蝦皮爬蟲時，在許多網路文章都會使用前面段落所使用的API來爬蟲，一樣可以取得資料，您可能會問那還何必使用Selenium來爬蟲呢？這不是殺雞焉用牛刀嗎？其實不然！

在作者自己的實驗中，若爬到約50頁左右的蝦皮頁面開始，蝦皮API封包內的json 資料就會開始有缺漏，明顯是蝦皮官方為了防止機器人行為而進行的防護行為；反之若使用Selenium 來進行滾動頁面，雖然速度會慢上許多，但經過實驗證得，這樣並不會被官方認定為機器人，因此蝦皮API封包內的json 資料還是會一樣完整。

由於在2022年底蝦皮的API更新，因此本爬蟲改使用[seleniumwire](https://pypi.org/project/selenium-wire/)來進行爬取，因此需要先安裝[seleniumwire](https://pypi.org/project/selenium-wire/)，安裝指令如下：
```
pip install selenium-wire
```

### 資料容器準備
整個爬蟲的過程會利用time.time() 的方法來計時，方便之後調整爬蟲的效率。另外準備了container_product 與container_comment 兩個變數。分別用來存放商品的詳細資料與每個商品對應的留言。
在2022/12的程式碼修整中，也將`options.add_argument('blink-settings=imagesEnabled=false') `加入程式碼，此舉能讓
```python
# 自動下載ChromeDriver
service = ChromeService(executable_path=ChromeDriverManager().install())

# 關閉通知提醒
options = webdriver.ChromeOptions()
prefs = {"profile.default_content_setting_values.notifications" : 2}
options.add_experimental_option("prefs",prefs)
# 不載入圖片，提升爬蟲速度
options.add_argument('blink-settings=imagesEnabled=false') 

# 開啟瀏覽器
driver = webdriver.Chrome(service=service, chrome_options=options)
time.sleep(random.randint(5,10))

# 開啟網頁
driver.get('https://shopee.tw/search?keyword=' + keyword )
time.sleep(random.randint(10,20))
```
### 開始每頁商品的爬蟲

![取得商品的示意圖](https://cdn-images-1.medium.com/max/1200/1*ChrqpHRuFbjWuPUHcos6xA.png)
我們在「基本參數設定」的步驟時已經設定了page 這個變數，這個變數決定我們要爬幾頁資料（一頁50個商品），因此這層的迴圈就是一頁一頁進去取得商品資料。
```python
for i in range(int(page)):

    # 準備用來存放資料的陣列
    itemid = []
    shopid =[]
    name = []
    brand = [] ＃以下變數太多因此省略，完整程式碼請見文章最下方...
    
    driver.get('https://shopee.tw/search?keyword=' + keyword + '&amp;page=' + str(i))
```

### 滾動頁面
![滾動時可以發現，商品資料會一一載入](https://cdn-images-1.medium.com/max/1200/1*3-2_YdQC_e18Gbgs_59q6g.png)
蝦皮也是屬於動態式的網站，因此畫面必須要向下滾動，才會觸發而取得所有50個商品資料。
```python
# 滾動頁面
    for scroll in range(6):
        driver.execute_script('window.scrollBy(0,1000)')
        time.sleep(2)
        
```
### 取得商品的「商品ID」、「商家ID」、「商品名稱」
![取得連結即可獲得商品的「商品ID」、「商家ID」、「商品名稱」](https://cdn-images-1.medium.com/max/1200/1*BNoPlFzIBjQSmqHr6x8JVQ.png")
由上圖可以發現，商品的「商品ID」、「商家ID」、「商品名稱」都在連結裏頭，因此會特別將標籤內的href 標籤爬下來。
```python
#取得商品內容
for item, thename in zip(driver.find_elements(by=By.XPATH, value='//*[@data-sqe="link"]'), driver.find_elements(by=By.XPATH, value='//*[@data-sqe="name"]')):
    #商品ID、商家ID、商品連結
    getID = item.get_attribute('href')
    theitemid = int((getID[getID.rfind('.')+1:getID.rfind('?')]))
    theshopid = int(getID[ getID[:getID.rfind('.')].rfind('.')+1 :getID.rfind('.')]) 
    link.append(getID)
    itemid.append(theitemid)
    shopid.append(theshopid)
    
    #商品名稱
    getname = thename.text.split('\n')[0]
    name.append(getname)
```
蝦皮商品的連結格式如下圖：
```https://shopee.tw/【商品的名稱】-i.【商品ID】.【店家ID】```

### 取得商品「價格」與「地區」
![資料有時會不完整](https://cdn-images-1.medium.com/max/1200/1*Y1VRc5ylic73qMGV1J4C4w.png)
商品的「價格」與「地區」資料因為有時會不完整，因此需要使用IF判斷式來防止程式出錯。取得的價格必須要以數字的型態儲存，方便之後資料計算方便，因此裡面所有的「非數值型資料」都需要取代掉。
```python
#價格
thecontent = item.text
thecontent = thecontent[(thecontent.find(getname)) + len(getname):]
thecontent = thecontent.replace('萬','000')
thecut = thecontent.split('\n')

if bool(re.search('市|區|縣|鄉|海外|中國大陸', thecontent)): #有時會沒有商品地點資料
    if bool(re.search('已售出', thecontent)): #有時會沒銷售資料
        if '出售' in thecut[-3][1:]:
            theprice = thecut[-4][1:]
        else:
            theprice = thecut[-3][1:]

    else:
        theprice = thecut[-2][1:]
else:
    if re.search('已售出', thecontent): #有時會沒銷售資料
        theprice = thecut[-2][1:]
    else:
        theprice = thecut[-1][1:]
        
theprice = theprice.replace('$','')
theprice = theprice.replace(',','')
theprice = theprice.replace('售','')
theprice = theprice.replace('出','')
theprice = theprice.replace(' ','')
if ' - ' in theprice:
    theprice = (int(theprice.split(' - ')[0]) +int(theprice.split(' - ')[1]))/2
if '-' in theprice:
    theprice = (int(theprice.split('-')[0]) +int(theprice.split('-')[1]))/2
price.append(int(theprice))
```
### 請求商品詳細資料
請求「設定商品的「商品資料」方法」步驟的方法，取得所有的Json資料，並整理到所有的變數中，方便等等轉乘DataFrame 資料型態。
```python
    for lnk, itid, shid in zip(link, itemid, shopid):
        #請求商品詳細資料
        itemDetail = goods_detail(url = lnk, item_id = itid, shop_id = shid)
        print('抓取： '+ str(itid) )
        if itemDetail == '':
            # 抓不到資料就全部塞空值
            print('抓不到商品詳細資料...\n')
            brand.append(None) #品牌
            stock.append(None) #存貨數量
            ctime.append(None) #上架時間
            # 資料過多因此省略，完整請至底部完整程式碼
            continue

        brand.append(itemDetail['brand']) #品牌
        stock.append(itemDetail['stock']) #存貨數量
        ctime.append(itemDetail['ctime']) #上架時間
        description.append(itemDetail['description']) #商品文案
        discount.append(itemDetail['discount']) #折數
        can_use_bundle_deal.append(itemDetail['can_use_bundle_deal']) #可否搭配購買
        can_use_wholesale.append(itemDetail['can_use_wholesale']) #可否大量批貨購買
        tier_variations.append(itemDetail['tier_variations']) #選項
        historical_sold.append(itemDetail['historical_sold']) #歷史銷售量
        is_cc_installment_payment_eligible.append(itemDetail['is_cc_installment_payment_eligible']) #可否分期付款
        is_official_shop.append(itemDetail['is_official_shop']) #是否官方賣家帳號
        is_pre_order.append(itemDetail['is_pre_order']) #是否可預購
        liked_count.append(itemDetail['liked_count']) #喜愛數量
```
### 取得商品「SKU」與「星等」
由於不一定每個商品都有「SKU」，因此在整理之前必須要先檢查是否有資料。
```python
#SKU
all_sku=[]
for sk in itemDetail['models']:
    all_sku.append(sk['name'])
SKU.append(all_sku) #SKU
shop_location.append(itemDetail['shop_location']) #商家地點
cmt_count.append(itemDetail['cmt_count']) #評價數量
five_star.append( itemDetail['item_rating']['rating_count'][5] ) #五星
four_star.append( itemDetail['item_rating']['rating_count'][4] ) #四星
three_star.append( itemDetail['item_rating']['rating_count'][3] ) #三星
two_star.append( itemDetail['item_rating']['rating_count'][2] ) #二星
one_star.append( itemDetail['item_rating']['rating_count'][1] ) #一星
rating_star.append(itemDetail['item_rating']['rating_star']) #評分
```
### 取得商品的「留言資料」
請求「設定商品的「留言資料」方法」步驟的方法，取得所有的Json資料。由於Json檔案中的['comment']、['model_name']，不一定每個留言都有，因此使用try error的方式來寫。
```python
# 消費者評論詳細資料
iteComment = goods_comments(item_id = theitemid, shop_id = theshopid)
userid = [] #使用者ID
anonymous = [] #是否匿名
commentTime = [] #留言時間
is_hidden = [] #是否隱藏
orderid = [] #訂單編號
comment_rating_star = [] #給星
comment = [] #留言內容
product_SKU = [] #商品規格

for comm in iteComment:
    userid.append(comm['userid'])
    anonymous.append(comm['anonymous'])
    commentTime.append(comm['ctime'])
    is_hidden.append(comm['is_hidden'])
    orderid.append(comm['orderid'])
    comment_rating_star.append(comm['rating_star'])
    try:
        comment.append(comm['comment'])
    except:
        comment.append(None)
    
    p=[]
    for pro in comm['product_items']:
        try:
            p.append(pro['model_name'])
        except:
            p.append(None)
            
    product_SKU.append(p)
```
### 整理所有「商品」與「留言」資料
整理上述的所有資料，並打包成DataFrame 資料格式。在整個For 迴圈的執行途中，為了防止任何突發意外，導致迴圈停止資料消失，因此在每一頁爬蟲結束時會儲存檔案一次，並整理成CSV檔案。
```python
commDic = {
     '商品ID':[ theitemid for x in range(len(iteComment)) ],
     '賣家ID':[ theshopid for x in range(len(iteComment)) ],
     '商品名稱':[ getname for x in range(len(iteComment)) ] # 資料過多因此省略，完整請至底部完整程式碼
     }
container_comment = pd.concat([container_comment,pd.DataFrame(commDic)], axis=0)
dic = {
     '商品ID':itemid,
     '賣家ID':shopid,
     '商品名稱':name # 資料過多因此省略，完整請至底部完整程式碼
     }
#資料整合
container_product = pd.concat([container_product,pd.DataFrame(dic)], axis=0)
#暫時存檔紀錄
container_product.to_csv('shopeeAPIData'+str(i+1)+'_Product.csv', encoding = ecode)
container_comment.to_csv('shopeeAPIData'+str(i+1)+'_Comment.csv', encoding = ecode)
print('目前累積商品： ' + str(len(container_product)) + ' 留言累積' + str(len(container_comment)))
time.sleep(random.randint(5,10))
```
### 存檔
經過了漫長的執行，最後儲存檔案，並結算這次執行的總時間。
```python
container_product.to_csv(keyword +'_商品資料.csv', encoding = ecode, index=False)
container_comment.to_csv(keyword +'_留言資料.csv', encoding = ecode, index=False)
tEnd = time.time()#計時結束
totalTime = int(tEnd - tStart)
minute = totalTime // 60
second = totalTime % 60
print('資料儲存完成，花費時間（約）： ' + str(minute) + ' 分 ' + str(second) + '秒')</code></pre>

driver.close()
```

### Debug日誌
#### 蝦皮V4 API阻擋爬蟲
> 再次感謝[許譽騰](https://www.facebook.com/profile.php?id=100002303638013)同學協助排除此bug！
在2022第四季，陸續有學員發現蝦皮的API請求商品資料時，會回傳以下json結果：
```json
{
    "challenge_type": 1,
    "captcha": {
        "scene": "crawler_item"
    },
    "captcha_extra_info": {
        "tracking_id": "95961fb7-6ad3-494e-80fb-13646c9dd347"
    },
    "action_type": 3,
    "error": 90309999,
    "is_customized": false,
    "is_login": false,
    "platform": 0,
    "report_extra_info": ""
}
```

看到「crawler_item」心頭一陣涼風，蝦皮是怎麼知道我是爬蟲的？而後開始排查header的參數，雖然新增了不少餐數與cookies，但最關鍵的參數莫過於以下：
```json
{
    "af-ac-enc-dat": "AAcyLjQuMS0yAAABhELlOE8AAAtvAkAAAAAAAAAAAOYyhFAbVQMMpIKa2+...太長了後面省略"
}
```
當我將這個參數改為null後重新請求，網頁就能夠顯示了，但仍然不會給我商品的json資料，看來在這其中，除了跟cookie有交互檢查外，也有包含商品的參數在裡面，因此這個參數仍然不可或缺。
```json
{
    "af-ac-enc-dat": "null"
}
```
最好的解決方法，是破解「af-ac-enc-dat」這個參數是如何生成的，但這條逆向工程的路遙不可及（至少對我是），正在手足無措時，收到[行銷搬進大程式](https://marketingliveincode.com/)的同學[許譽騰](https://www.facebook.com/profile.php?id=100002303638013)來信，建議我可以使用[seleniumwire](https://pypi.org/project/selenium-wire/)的方式抓取生成出來的「af-ac-enc-dat」，好方法！立馬手刀實驗！
經過實驗，發現確實能撈到生成出來的「af-ac-enc-dat」，但既然能生成出來的「af-ac-enc-dat」，代表整個封包也一定被撈到了？（有玩過[wireshark](https://www.wireshark.org/download.html)的讀者一定聽得懂）。經過幾個小時的摸索，發現[seleniumwire](https://pypi.org/project/selenium-wire/)果真有收錄封包的內容在「request.response.body」裡面，才會以以下的程式碼，來取得封包內容：
```python
getPacket = ''
for request in driver.requests:
    if request.response:
        # 挑出商品詳細資料的json封包
        if 'https://shopee.tw/api/v4/item/get?itemid=' + str(item_id) + '&shopid=' + str(shop_id) in request.url:
            # 此封包是有壓縮的，因此需要解壓縮
            getPacket = zlib.decompress(
                request.response.body,
                16+zlib.MAX_WBITS
                )
            break
```
