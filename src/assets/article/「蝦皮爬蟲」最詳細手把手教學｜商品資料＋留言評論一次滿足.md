# 「蝦皮爬蟲」最詳細手把手教學｜商品資料＋留言評論一次滿足
## 課程介紹
> [建議您可以先看「Json爬蟲教學－Google趨勢搜尋」課程，會更了解蝦皮如何爬喔！](/class?c=3&a=8)
蝦皮橫掃台灣的店商產業，搶佔市佔率的結果，當年的Yahoo拍賣、露天拍賣都不是對手。或許您或您的企業也有經營電商生意，對於蝦皮的進軍肯定是膽戰心驚。若換個角度來想，蝦皮也是統整了許多「競爭對手」的平台，若我們想模仿產業內的行銷方式，或者想窺探競爭對手的動態；又或者是，你的商品單價過高，不適合在蝦皮上架，但蝦皮內的某些消費者需求，或許能應用到自己的品牌，因此爬下蝦皮對於產業的窺探有不可忽視的效果。

## 可交付成果
許多網路的文章都有蝦皮爬蟲的教學，但實際執行時就會發現爬蟲被官方擋住了。本文章經實驗最高可得到7萬比的商品資料，以及這些商品所有的留言更是破好幾百萬，這個結果是因為作者主動停下爬蟲（因為檔案過大），因此利用本文章的工具取得蝦皮中單一品項的所以資料是有可能的。

可交付成果有兩個主要檔案，命名方式分別為「<strong>XXX_商品資料.csv</strong>」與「<strong>XXX_留言資料.csv</strong>」，其內容為每個商品的所有資訊與留言資料，爬蟲的過程依照需求的不同，可能會爬到好幾夜之久，為了怕途中任何意外導致功歸一匱，因此每爬完一頁（50個商品）就會存檔記錄一次。
![成果：留言資料](https://cdn-images-1.medium.com/max/1200/1*M1OiQQ0O-599iC9Pem-aug.png)
![成果：商品資料](https://cdn-images-1.medium.com/max/1200/1*rSAciO0mzW_fXvIrcp_9Hg.png)

## 開始爬蟲
### 基本參數設定
本次的課程以服裝產業的「花襯衫」這個品項為例子，由於產品過多，因此先設定爬下50頁商品資料作為範例。至於變數my_headers內的資料，為封包需要用到的Header資料為何會那麼長，可以參考「[Html爬蟲Post實戰－UberEats](/class?c=3&a=88)」課程中有詳細的解說。

```python
keyword = '花襯衫'
page = 50
ecode = 'utf-8-sig'
my_headers = {'authority' : 'shopee.tw',
     'method': 'GET',
     'path': '/api/v1/item_detail/?item_id=1147052312&amp;shop_id=17400098',
     'scheme': 'https', #以下過多省略，完整請至底部完整程式碼
}
```
### 設定商品的「留言資料」方法
在課程「[Json爬蟲教學－Google趨勢搜尋](/class?c=3&a=8)」中已經教您如何取得API資料，再利用requests.get()方法取得文字內容後，需要將一些特殊的字元進行取代後，才能利用 json.loads 轉成Json資料格式。以下方法給予「商品ID」與「店家ID」，即可取得該商品的買家留言。

```python
# 進入每個商品，抓取買家留言
def goods_comments(item_id, shop_id):
    # url = 'https://shopee.tw/api/v2/item/get_ratings?itemid='+ str(item_id) + '&amp;shop_id=' + str(shop_id) + '&amp;offset=0&amp;limit=200&amp;flag=1&amp;filter=0'
    url = 'https://shopee.tw/api/v2/item/get_ratings?filter=0&amp;flag=1&amp;itemid='+ str(item_id) + '&amp;limit=50&amp;offset=0&amp;shopid=' + str(shop_id) + '&amp;type=0'
    r = requests.get(url,headers = my_headers)
    st= r.text.replace("\\n","^n")
    st=st.replace("\\t","^t")
    st=st.replace("\\r","^r")
    
    gj=json.loads(st)
    return gj['data']['ratings']</code></pre>
```
### 設定商品的「商品資料」方法
以下方法給予「商品ID」與「店家ID」，即可取得該商品的詳細資料。 該資料其實在商品搜尋頁的API也可以取得，但在商品搜尋頁的API 中沒有 商品文案、SKU（Stock Keeping Unit，單品項管理）等資料，若有需要用到該資料，還是需要使用此方法。
```python
# 進入每個商品，抓取賣家更細節的資料（商品文案、SKU）
def goods_detail(item_id, shop_id):
    url = 'https://shopee.tw/api/v4/item/get?itemid=' + str(item_id) + '&amp;shopid=' + str(shop_id)
    r = requests.get(url,headers = my_headers)
    st= r.text.replace("\\n","^n")
    st=st.replace("\\t","^t")
    st=st.replace("\\r","^r")
    
    gj=json.loads(st)
    return gj

```
### 為何使用Selenium與設定
接下來使用Selenium來進入蝦皮網站，若還沒了解 Selenium 的您，可以參考「[Selenium介紹](/class?c=3&a=92)」課程。或許您在搜尋許多蝦皮爬蟲時，在許多網路文章都會使用前面段落所使用的API來爬蟲，一樣可以取得資料，您可能會問那還何必使用Selenium來爬蟲呢？這不是殺雞焉用牛刀嗎？其實不然！

在作者自己的實驗中，若爬到約50頁左右的蝦皮頁面開始，蝦皮API封包內的json 資料就會開始有缺漏，明顯是蝦皮官方為了防止機器人行為而進行的防護行為；反之若使用Selenium 來進行滾動頁面，雖然速度會慢上許多，但經過實驗證得，這樣並不會被官方認定為機器人，因此蝦皮API封包內的json 資料還是會一樣完整。
```python
# 設定基本參數
desired_capabilities = DesiredCapabilities.PHANTOMJS.copy()
#此處必須換成自己電腦的User-Agent
desired_capabilities['phantomjs.page.customHeaders.User-Agent'] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'

# PhantomJS driver 路徑 似乎只能絕對路徑
driver = webdriver.PhantomJS(executable_path = 'phantomjs', desired_capabilities=desired_capabilities)

```
### 資料容器準備
整個爬蟲的過程會利用time.time() 的方法來計時，方便之後調整爬蟲的效率。另外準備了container_product 與container_comment 兩個變數。分別用來存放商品的詳細資料與每個商品對應的留言。
```python
# 自動下載ChromeDriver
service = ChromeService(executable_path=ChromeDriverManager().install())

# 關閉通知提醒
chrome_options = webdriver.ChromeOptions()
prefs = {"profile.default_content_setting_values.notifications" : 2}
chrome_options.add_experimental_option("prefs",prefs)

# 開啟瀏覽器
driver = webdriver.Chrome(service=service, chrome_options=chrome_options)
time.sleep(5)

# 開啟網頁
driver.get('https://shopee.tw/search?keyword=' + keyword )
time.sleep(10)
```
### 開始每頁商品的爬蟲

![取得商品的示意圖](https://cdn-images-1.medium.com/max/1200/1*ChrqpHRuFbjWuPUHcos6xA.png)
我們在「基本參數設定」的步驟時已經設定了page 這個變數，這個變數決定我們要爬幾頁資料（一頁50個商品），因此這層的迴圈就是一頁一頁進去取得商品資料。
```python
for i in range(int(page)):

    # 準備用來存放資料的陣列
    itemid = []
    shopid =[]
    name = []
    brand = [] ＃以下變數太多因此省略，完整程式碼請見文章最下方...
    
    driver.get('https://shopee.tw/search?keyword=' + keyword + '&amp;page=' + str(i))
```

### 滾動頁面
![滾動時可以發現，商品資料會一一載入](https://cdn-images-1.medium.com/max/1200/1*3-2_YdQC_e18Gbgs_59q6g.png)
蝦皮也是屬於動態式的網站，因此畫面必須要向下滾動，才會觸發而取得所有50個商品資料。
```python
# 滾動頁面
    for scroll in range(6):
        driver.execute_script('window.scrollBy(0,1000)')
        time.sleep(2)
        
```
### 取得商品的「商品ID」、「商家ID」、「商品名稱」
![取得連結即可獲得商品的「商品ID」、「商家ID」、「商品名稱」](https://cdn-images-1.medium.com/max/1200/1*BNoPlFzIBjQSmqHr6x8JVQ.png")
由上圖可以發現，商品的「商品ID」、「商家ID」、「商品名稱」都在連結裏頭，因此會特別將標籤內的href 標籤爬下來。
```python
#取得商品內容
for item, thename in zip(driver.find_elements_by_xpath('//*[@data-sqe="link"]'), driver.find_elements_by_xpath('//*[@data-sqe="name"]')):
    #商品ID、商家ID
    getID = item.get_attribute('href')
    theitemid = int((getID[getID.rfind('.')+1:]))
    theshopid = int(getID[ getID[:getID.rfind('.')].rfind('.')+1 :getID.rfind('.')]) 
    itemid.append(theitemid)
    shopid.append(theshopid)
        
    #商品名稱
    getname = thename.text.split('\n')[0]
    print('抓取： '+getname)
    name.append(getname)
```
蝦皮商品的連結格式如下圖：
```https://shopee.tw/【商品的名稱】-i.【商品ID】.【店家ID】```

### 取得商品「價格」與「地區」
![資料有時會不完整](https://cdn-images-1.medium.com/max/1200/1*Y1VRc5ylic73qMGV1J4C4w.png)
商品的「價格」與「地區」資料因為有時會不完整，因此需要使用IF判斷式來防止程式出錯。取得的價格必須要以數字的型態儲存，方便之後資料計算方便，因此裡面所有的「非數值型資料」都需要取代掉。
```python
#價格
thecontent = item.text
thecontent = thecontent[(thecontent.find(getname)) + len(getname):]
thecontent = thecontent.replace('萬','000')
thecut = thecontent.split('\n')
if re.search('市|區|縣|鄉|海外|中國大陸', thecontent): #有時會沒有商品地點資料
     if re.search('已售出', thecontent): #有時會沒銷售資料
          if '出售' in thecut[-3][1:]:
              theprice = thecut[-4][1:]
          else:
              theprice = thecut[-3][1:]
     else:
          theprice = thecut[-2][1:]
else:
     if re.search('已售出', thecontent): #有時會沒銷售資料
          theprice = thecut[-2][1:]
     else:
          theprice = thecut[-1][1:]
          
theprice = theprice.replace('$','')
theprice = theprice.replace(',','')
theprice = theprice.replace('售','')
theprice = theprice.replace('出','')
theprice = theprice.replace(' ','')
if ' - ' in theprice:
     theprice = (int(theprice.split(' - ')[0]) +int(theprice.split(' - ')[1]))/2
if '-' in theprice:
     theprice = (int(theprice.split('-')[0]) +int(theprice.split('-')[1]))/2
price.append(int(theprice))
```
### 請求商品詳細資料
請求「設定商品的「商品資料」方法」步驟的方法，取得所有的Json資料，並整理到所有的變數中，方便等等轉乘DataFrame 資料型態。
```python
#請求商品詳細資料
itemDetail = goods_detail(item_id = theitemid, shop_id = theshopid)['data']

brand.append(itemDetail['brand']) #品牌
stock.append(itemDetail['stock']) #存貨數量
ctime.append(itemDetail['ctime']) #上架時間
description.append(itemDetail['description']) #商品文案
discount.append(itemDetail['discount']) #折數
can_use_bundle_deal.append(itemDetail['can_use_bundle_deal']) #可否搭配購買
can_use_wholesale.append(itemDetail['can_use_wholesale']) #可否大量批貨購買
tier_variations.append(itemDetail['tier_variations']) #選項
historical_sold.append(itemDetail['historical_sold']) #歷史銷售量
is_cc_installment_payment_eligible.append(itemDetail['is_cc_installment_payment_eligible']) #可否分期付款
is_official_shop.append(itemDetail['is_official_shop']) #是否官方賣家帳號
is_pre_order.append(itemDetail['is_pre_order']) #是否可預購
liked_count.append(itemDetail['liked_count']) #喜愛數量
```
### 取得商品「SKU」與「星等」
由於不一定每個商品都有「SKU」，因此在整理之前必須要先檢查是否有資料。
```python
#SKU
all_sku=[]
for sk in itemDetail['models']:
    all_sku.append(sk['name'])
SKU.append(all_sku) #SKU
shop_location.append(itemDetail['shop_location']) #商家地點
cmt_count.append(itemDetail['cmt_count']) #評價數量
five_star.append( itemDetail['item_rating']['rating_count'][5] ) #五星
four_star.append( itemDetail['item_rating']['rating_count'][4] ) #四星
three_star.append( itemDetail['item_rating']['rating_count'][3] ) #三星
two_star.append( itemDetail['item_rating']['rating_count'][2] ) #二星
one_star.append( itemDetail['item_rating']['rating_count'][1] ) #一星
rating_star.append(itemDetail['item_rating']['rating_star']) #評分
```
### 取得商品的「留言資料」
請求「設定商品的「留言資料」方法」步驟的方法，取得所有的Json資料。由於Json檔案中的['comment']、['model_name']，不一定每個留言都有，因此使用try error的方式來寫。
```python
# 消費者評論詳細資料
iteComment = goods_comments(item_id = theitemid, shop_id = theshopid)
userid = [] #使用者ID
anonymous = [] #是否匿名
commentTime = [] #留言時間
is_hidden = [] #是否隱藏
orderid = [] #訂單編號
comment_rating_star = [] #給星
comment = [] #留言內容
product_SKU = [] #商品規格

for comm in iteComment:
    userid.append(comm['userid'])
    anonymous.append(comm['anonymous'])
    commentTime.append(comm['ctime'])
    is_hidden.append(comm['is_hidden'])
    orderid.append(comm['orderid'])
    comment_rating_star.append(comm['rating_star'])
    try:
        comment.append(comm['comment'])
    except:
        comment.append(None)
    
    p=[]
    for pro in comm['product_items']:
        try:
            p.append(pro['model_name'])
        except:
            p.append(None)
            
    product_SKU.append(p)
```
### 整理所有「商品」與「留言」資料
整理上述的所有資料，並打包成DataFrame 資料格式。在整個For 迴圈的執行途中，為了防止任何突發意外，導致迴圈停止資料消失，因此在每一頁爬蟲結束時會儲存檔案一次，並整理成CSV檔案。
```python
commDic = {
     '商品ID':[ theitemid for x in range(len(iteComment)) ],
     '賣家ID':[ theshopid for x in range(len(iteComment)) ],
     '商品名稱':[ getname for x in range(len(iteComment)) ] # 資料過多因此省略，完整請至底部完整程式碼
     }
container_comment = pd.concat([container_comment,pd.DataFrame(commDic)], axis=0)
dic = {
     '商品ID':itemid,
     '賣家ID':shopid,
     '商品名稱':name # 資料過多因此省略，完整請至底部完整程式碼
     }
#資料整合
container_product = pd.concat([container_product,pd.DataFrame(dic)], axis=0)
#暫時存檔紀錄
container_product.to_csv('shopeeAPIData'+str(i+1)+'_Product.csv', encoding = ecode)
container_comment.to_csv('shopeeAPIData'+str(i+1)+'_Comment.csv', encoding = ecode)
print('目前累積商品： ' + str(len(container_product)) + ' 留言累積' + str(len(container_comment)))
time.sleep(random.randint(5,10))
```
### 存檔
經過了漫長的執行，最後儲存檔案，並結算這次執行的總時間。
```python
container_product.to_csv(keyword +'_商品資料.csv', encoding = ecode, index=False)
container_comment.to_csv(keyword +'_留言資料.csv', encoding = ecode, index=False)
tEnd = time.time()#計時結束
totalTime = int(tEnd - tStart)
minute = totalTime // 60
second = totalTime % 60
print('資料儲存完成，花費時間（約）： ' + str(minute) + ' 分 ' + str(second) + '秒')</code></pre>

```



## Debug日誌
### 蝦皮V4 API阻擋爬蟲
> 再次感謝[許譽騰](https://www.facebook.com/profile.php?id=100002303638013)同學協助排除此bug！
在2022第四季，陸續有學員發現蝦皮的API請求商品資料時，會回傳以下json結果：
```json
{
    "challenge_type": 1,
    "captcha": {
        "scene": "crawler_item"
    },
    "captcha_extra_info": {
        "tracking_id": "95961fb7-6ad3-494e-80fb-13646c9dd347"
    },
    "action_type": 3,
    "error": 90309999,
    "is_customized": false,
    "is_login": false,
    "platform": 0,
    "report_extra_info": ""
}
```

看到「crawler_item」心頭一陣涼風，蝦皮是怎麼知道我是爬蟲的？而後開始排查header的參數，雖然新增了不少餐數與cookies，但最關鍵的參數莫過於以下：
```json
{
    "af-ac-enc-dat": "AAcyLjQuMS0yAAABhELlOE8AAAtvAkAAAAAAAAAAAOYyhFAbVQMMpIKa2+...太長了後面省略"
}
```
當我將這個參數改為null後重新請求，網頁就能夠顯示了，但仍然不會給我商品的json資料，看來在這其中，除了跟cookie有交互檢查外，也有包含商品的參數在裡面，因此這個參數仍然不可或缺。
```json
{
    "af-ac-enc-dat": "null"
}
```
最好的解決方法，是破解「af-ac-enc-dat」這個參數是如何生成的，但這條逆向工程的路遙不可及（至少對我是），正在手足無措時，收到[行銷搬進大程式](https://marketingliveincode.com/)的同學[許譽騰](https://www.facebook.com/profile.php?id=100002303638013)來信，建議我可以使用[seleniumwire](https://pypi.org/project/selenium-wire/)的方式抓取生成出來的「af-ac-enc-dat」，好方法！立馬手刀實驗！
經過實驗，發現確實能撈到生成出來的「af-ac-enc-dat」，但既然能生成出來的「af-ac-enc-dat」，代表整個封包也一定被撈到了？（有玩過[wireshark](https://www.wireshark.org/download.html)的讀者一定聽得懂）。經過幾個小時的摸索，發現[seleniumwire](https://pypi.org/project/selenium-wire/)果真有收錄封包的內容在「request.response.body」裡面，才會以以下的程式碼，來取得封包內容：
```python
getPacket = ''
for request in driver.requests:
    if request.response:
        # 挑出商品詳細資料的json封包
        if 'https://shopee.tw/api/v4/item/get?itemid=' + str(item_id) + '&shopid=' + str(shop_id) in request.url:
            # 此封包是有壓縮的，因此需要解壓縮
            getPacket = zlib.decompress(
                request.response.body,
                16+zlib.MAX_WBITS
                )
            break
```


### 蝦皮防爬蟲機制升級，加入機器人驗證
> 2023年起，在蝦皮搜尋商品前，都會被要求一定要先登入蝦皮。並且開始有同學爬到一半，被要求機器人驗證

這個狀況是非常困擾的。首先是搜尋前輩要求一定要登入蝦皮。此舉很明顯是要「實名制」，當登入後若檢測到該帳號的行為不尋常，此帳號並不會直接封鎖，而是列為警示，從此您的帳號在瀏覽商品的途中，很常會被要求進行機器人驗證。

機器人驗證在現在AI的大環境下是有解方的，但不管事使用一些外部AI服務，或者是自己訓練一個AI來破解機器人驗證，都是成本非常高的一件事。因此我們也並沒有要正面迎戰機器人驗證的意思。

應對這次蝦皮的防禦升級，我將整個程式碼的邏輯大修一番，主要是加上了「資料已完整爬取」欄位。整個爬蟲的邏輯會變成：
1. 手動登入蝦皮。
2. 搜尋關鍵字相關商品，先記錄每一個商品的連結，並都標註「尚未爬取」。
3. 利用前一步完成的結果，美個商品的連結進去抓資料，抓成功會標記為「已經爬取」。
4. 最後，使用每個商品的ID，去抓該商品的買賣留言。 
